{
 "metadata": {
  "name": "",
  "signature": "sha256:5439e400c3d4b7ad04aca160cbe8e9848f30395b90921e8664ad255e59b99cf0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nibabel as nib\n",
      "import numpy as np\n",
      "import scipy.ndimage\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.image as mpimg\n",
      "import matplotlib as mpl\n",
      "from scipy import stats\n",
      "from scipy import ndimage\n",
      "import glob\n",
      "import csv\n",
      "import os\n",
      "import re\n",
      "import math\n",
      "import sys\n",
      "import plotly.plotly as py\n",
      "from plotly.graph_objs import *\n",
      "from pylab import *\n",
      "\n",
      "\n",
      "import sklearn\n",
      "from sklearn import svm\n",
      "from sklearn import datasets\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.cross_validation import cross_val_score \n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "from sklearn.metrics import roc_curve, auc\n",
      "\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###---------------------------------------------------------###\n",
      "###-----Merge the 2 BIFs scales in one feature vector-------###\n",
      "###---------------------------------------------------------###\n",
      "\n",
      "'''\n",
      "Multi-scale sigma scales:\n",
      "\n",
      "0.0004mm\n",
      "0.0006mm\n",
      "\n",
      "Multi-scale LBP:\n",
      "E1 = LBP8-1\n",
      "E2 = LBP16-2\n",
      "E3 = LBP24-3\n",
      "E4 = LBP8-1_16-2\n",
      "E5 = LBP8-1_24-3\n",
      "E6 = LBP16-2_24-3\n",
      "E7 = LBP8-1_16-2_24-3\n",
      "\n",
      "'''\n",
      "##------ Load E1, E2, E3 -------##\n",
      "E1_link1 = '/Volumes/vph-prism5/Sara/SPIE16/Experiments_v2.0/lightLines/LightLines_LBP_8_1_features_0.0004mm.csv'\n",
      "E2_link1 = '/Volumes/vph-prism5/Sara/SPIE16/Experiments_v2.0/lightLines/LightLines_LBP_16_2_features_0.0004mm.csv'\n",
      "E3_link1 = '/Volumes/vph-prism5/Sara/SPIE16/Experiments_v2.0/lightLines/LightLines_LBP_24_3_features_0.0004mm.csv'\n",
      "\n",
      "E1_link2 = '/Volumes/vph-prism5/Sara/SPIE16/Experiments_v2.0/lightLines/LightLines_LBP_8_1_features_0.0006mm.csv'\n",
      "E2_link2 = '/Volumes/vph-prism5/Sara/SPIE16/Experiments_v2.0/lightLines/LightLines_LBP_16_2_features_0.0006mm.csv'\n",
      "E3_link2 = '/Volumes/vph-prism5/Sara/SPIE16/Experiments_v2.0/lightLines/LightLines_LBP_24_3_features_0.0006mm.csv'\n",
      "\n",
      "#================#\n",
      "E1_1 = []\n",
      "E1_LBP1 = open(E1_link1)\n",
      "E1_LBP1.readline()  # skip the header\n",
      "LBPreader1 = csv.reader(E1_LBP1)\n",
      "for row in LBPreader1:\n",
      "    E1_1.append(row)\n",
      "\n",
      "E1_2 = []\n",
      "E1_LBP2 = open(E1_link2)\n",
      "E1_LBP2.readline()  # skip the header\n",
      "LBPreader2 = csv.reader(E1_LBP2)\n",
      "for row in LBPreader2:\n",
      "    E1_2.append(row)\n",
      "\n",
      "E1=[]\n",
      "for i in range(len(E1_1)):\n",
      "    E1.append(E1_1[i]+E1_2[i])\n",
      "\n",
      "E1_LBP_header = np.arange(len(E1_1[0])).tolist()\n",
      "merged_E1_LBP_Header = E1_LBP_header + E1_LBP_header\n",
      "print \"Header:\"\n",
      "print merged_E1_LBP_Header\n",
      "\n",
      "file = open('/Volumes/vph-prism5/Sara/SPIE16/Experiments_v2.0/lightLines/E1_2scales.csv', \"wb\")\n",
      "writer = csv.writer(file)\n",
      "writer.writerow(merged_E1_LBP_Header)\n",
      "writer.writerows(E1)\n",
      "file.close()\n",
      "\n",
      "#================#\n",
      "E2_1 = []\n",
      "E2_LBP1 = open(E2_link1)\n",
      "E2_LBP1.readline()  # skip the header\n",
      "LBPreader1 = csv.reader(E2_LBP1)\n",
      "for row in LBPreader1:\n",
      "    E2_1.append(row)\n",
      "\n",
      "E2_2 = []\n",
      "E2_LBP2 = open(E2_link2)\n",
      "E2_LBP2.readline()  # skip the header\n",
      "LBPreader2 = csv.reader(E2_LBP2)\n",
      "for row in LBPreader2:\n",
      "    E2_2.append(row)\n",
      "\n",
      "E2=[]\n",
      "for i in range(len(E2_1)):\n",
      "    E2.append(E2_1[i]+E2_2[i])\n",
      "    \n",
      "E2_LBP_header = np.arange(len(E2_1[0])).tolist()\n",
      "merged_E2_LBP_Header = E2_LBP_header + E2_LBP_header\n",
      "print \"Header:\"\n",
      "print merged_E2_LBP_Header\n",
      "\n",
      "file = open('/Volumes/vph-prism5/Sara/SPIE16/Experiments_v2.0/lightLines/E2_2scales.csv', \"wb\")\n",
      "writer = csv.writer(file)\n",
      "writer.writerow(merged_E2_LBP_Header)\n",
      "writer.writerows(E2)\n",
      "file.close()\n",
      "\n",
      "    \n",
      "#================#    \n",
      "E3_1 = []\n",
      "E3_LBP1 = open(E3_link1)\n",
      "E3_LBP1.readline()  # skip the header\n",
      "LBPreader3 = csv.reader(E3_LBP1)\n",
      "for row in LBPreader3:\n",
      "    E3_1.append(row)\n",
      "\n",
      "E3_2 = []\n",
      "E3_LBP2 = open(E3_link2)\n",
      "E3_LBP2.readline()  # skip the header\n",
      "LBPreader3 = csv.reader(E3_LBP2)\n",
      "for row in LBPreader3:\n",
      "    E3_2.append(row)\n",
      "\n",
      "E3=[]\n",
      "for i in range(len(E3_1)):\n",
      "    E3.append(E3_1[i]+E3_2[i])\n",
      "\n",
      "E3_LBP_header = np.arange(len(E3_1[0])).tolist()\n",
      "merged_E3_LBP_Header = E3_LBP_header + E3_LBP_header\n",
      "print \"Header:\"\n",
      "print merged_E3_LBP_Header\n",
      "\n",
      "file = open('/Volumes/vph-prism5/Sara/SPIE16/Experiments_v2.0/lightLines/E3_2scales.csv', \"wb\")\n",
      "writer = csv.writer(file)\n",
      "writer.writerow(merged_E3_LBP_Header)\n",
      "writer.writerows(E3)\n",
      "file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Header:\n",
        "[0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
        "Header:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
        "Header:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}